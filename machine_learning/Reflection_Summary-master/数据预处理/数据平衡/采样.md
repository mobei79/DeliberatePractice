# 为什么要对数据进行采样平衡
- 下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果
- 上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练
    - 比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况

# 是否一定需要对原始数据进行采样平衡
否。
- 采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降
- 采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合

# 有哪些常见的采样方法？
- 随机采样
    - 无放回的简单抽样：每条样本被采到的概率相等且都为1/N
    - 有放回的简单抽样：每条样本可能多次被选中
    - 上采样：即合理地增加少数类的样本
    - 下采样：欠抽样技术是将数据从原始数据集中移除
    - 平衡采样：考虑正负样本比
    - 分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样
    - 整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集
- 合成采样
相对于采样随机的方法进行上采样, 还有两种比较流行的上采样的改进方式： 
    - SMOTE
        - x_new = x + rand(0,1) * (x′−x)
        - **带来新样本的同时有可能造成不同类别样本之间的重合**
        - Borderline-SMOTE为了解决上面的问题，在x_new生成之前，会先判断x这个点是否周围都是同类别的点
    - ADASYN
        - 同上，也是在构造样本点的过程中考虑了正负样本比
- 平衡欠采样        
    - EasyEnsemble，利用模型融合的方法（Ensemble）
        - 少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合
    - BalanceCascade，利用模型融合的方法（Boost）
        - 每次剔除预测正确的多数样本，加入新的未预测的多数样本
    - NearMiss
        - 选择离各种情况下的少数样本位置最远的多数样本进行训练                

# 能否避免采样？
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。

# 你平时怎么用采样方法？
尽量避免使用合成采样的方式去做数据填充，总结如下：
- 由于项目中时间的充裕问题，填充的结果往往是正负样本交叠且无感知的，会干扰分类器
- 通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀
- 合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合