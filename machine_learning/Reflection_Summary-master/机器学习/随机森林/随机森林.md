# 解释下随机森林?
- 随机森林=bagging+决策树
- 随机：特征选择随机+数据采样随机
    - 特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机
    - 每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**
    - 数据采样，是有放回的采样
        - 1个样本**未被选到**的概率为p = (1 - 1/N)^N = 1/e，即为OOB
- 森林：多决策树组合
    - 可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票

# 随机森林用的是什么树？
CART树

# 随机森林的生成过程？
- 生成单棵决策树
    - 随机选取样本
    - 从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂
    - 不需要剪枝，直到该节点的所有训练样例都属于同一类
- 生成若干个决策树

# 解释下随机森林节点的分裂策略？
Gini系数

在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)

# 随机森林的损失函数是什么？
- 分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益
- 回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae
- 参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)

# 为了防止随机森林过拟合可以怎么做?
- 增加树的数量
- 增加叶子结点的数据数量
- bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7)
- 随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高

# 随机森林特征选择的过程？
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无
    - 通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）
    - 是使用uniform或者gaussian抽取随机值替换原特征
    
# 是否用过随机森林，有什么技巧?
- 除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForest-subspace变成randomForest-combination

# RF的参数有哪些，如何调参？
要调整的参数主要是 n_estimators和max_features 
- n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好
- max_features是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多
    - 回归：max_features = n_features
    - 分类：max_features = sqrt(n_features)

其他参数中
- class_weight也可以调整正负样本的权重
-  max_depth = None 和 min_samples_split = 2 结合，为不限制生成一个不修剪的完全树
 
# RF的优缺点 ？
- 优点:
    - 不同决策树可以由不同主机并行训练生成，效率很高
    - 随机森林算法继承了CART的优点
    - 将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题
- 缺点：
    - 没有严格数学理论支持
