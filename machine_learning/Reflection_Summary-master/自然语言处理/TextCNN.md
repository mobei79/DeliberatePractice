# 讲一下textcnn

简要描述一些embedding，conv，pooling，fc的每个过程，这边建议着重描述三个点：

- embedding是可以用pretrain_embedded的vector，常用的比如sent2vec，word2vec，GloVe
- ⼀维卷积层核的选择和filter_size的选择
- pooling的选择max的思考

流程是：

- 定义多个⼀维卷积核，并使⽤这些卷积核对输⼊分别做卷积计算。
- 对输出的所有通道分别做时序最⼤池化，再将这些通道的池化输出值连结为向量。
- 通过全连接层将连结后的向量变换为有关各类别的输出。这⼀步可以使⽤丢弃层应对过拟合。

# textCNN中核的作用？

ngram

# max-pooling选择的目的？

抽取关键信息

# textcnn和fasttext区别？

核心差别：textCNN的思路来自cv，抽取全局内的每个词附近的局部信息进行合并；fasttext的思路来自word2vec，充分利用了窗口内的信息
核心问题：均丢失了结构信息，都是类似bow这种类型的网络

# 如果你知道上面说的核心问题，那么有什么解决方案吗？

- 可以尝试k-max pooling做一些优化，k-max pooling针对每个卷积核都不只保留最大的值，他保留前k个最大值，并且保留这些值出现的顺序，也即按照文本中的位置顺序来排列这k个最大值
- TextRNN

# 为什么卷积核都不大？且常见都都是奇数？

- same padding时的处理时，补充的zero padding左右分配不均匀；valid无所谓
- 奇数相对于偶数，有中心点，可以更有效的提取中心两侧的信息

# 为什么不建议用句长作为核大小？

核对应的是n元语法的概念，句长相当于用了一整个句子进行语义提取，会引入噪声；其实核的大小也不是越小越小越容易丢失丢失信息，比如存在<中国，人民，共和国>，核为2的话，就误解了该句的意思，容易把这句话认为是讲中国人的，其实是讲中国的。对于句子较长的文本，则应选择大一些

*这个问题在我面试阿里的时候被问过一次，其实我也没有标准答案，有答案的同学可以分享一下*

# padding是不是对最后结果没有影响？

不一定，maxpooling没有影响，其他pooling会有影响，max只抽取了最大项padding影响到的项都被过滤了

****
textcnn是nlp中非常基础的算法，地位与lda近似，不建议有答不出来的点