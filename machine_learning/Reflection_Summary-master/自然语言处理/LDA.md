# 详述LDA原理？
- 从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)
    - 多项式分布的共轭分布是狄利克雷分布
    - 二项式分布的共轭分布是Beta分布
- 从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)
- 从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)
- 从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg)
- 文档里某个单词出现的概率可以用公式表示：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b9y6avdtj306e01jdfo.jpg)
- 采用EM方法修正词-主题矩阵+主题-文档矩阵直至收敛

# LDA中的主题矩阵如何计算?词分布矩阵如何计算？
这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点
- 吉布斯采样
	- 先随机给每个词附上主题
	- 因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布
	- 有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布
	- 根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题
	- 收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布

- 通常会引申出如下几个问题：
	- 吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）
	- MCMC中什么叫做蒙特卡洛方法？
		- 通常用于求概率密度的积分
		- 用已知分布去评估未知分布
		- reject-acpect过程
	- 马尔科夫链收敛性质？
		- 非周期性，不能出现死循环
		- 连通性，不能有断点
	- MCMC中什么叫做马尔科夫链采样过程？
		- 先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵
		- 再根据平稳矩阵后的条件概率p(x/xt)得到平稳分布的样本集(xn+1,xn+2...)
	- 给定平稳矩阵如何得到概率分布样本集？
		- M-C采样
			- 给定任意的转移矩阵Q，已知π(i)p(i,j) = π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j) = π(j)Q(j,i)a(j,i)
			- 根据Q的条件概率Q(x/xt)得到xt+1
			- u~uniform
			- u<π(xt+1)Q(xt+1,xt) 则accept，就和蒙特模拟一样否则xt+1 = xt
			- (xt,xt+1...)代表着我们的分布样本集
		- M-H采样
			- 左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度
		- Gibbs采样
			- 同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样
	- 什么叫做坐标转换采样？
		- 平面上任意两点满足细致平稳条件π(A)P(A->B) = π(B)P(B->A)
		- 从条件概率分布P(x2|x(t)1)中采样得到样本x(t+1)2
		- 从条件概率分布P(x1|x(t+1)2)中采样得到样本x(t+1)1
		- 其为一对样本，有点像Lasso回归中的固定n-1维特征求一维特征求极值的思路

- 变分推断EM算法
	- 整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta
	- 变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**
		- 实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵
	- EM过程
		- E：最小化相对熵，偏导为0得到变分参数
		- M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值

# LDA的共轭分布解释下?
以多项式分布-狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布


# PLSA和LDA的区别?
- LDA是加了狄利克雷先验的PLSA
- PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的
- LDA是贝叶斯思想，PLSA是MLE

# 怎么确定LDA的topic个数
- 对文档d属于哪个topic有多不确定，这个不确定程度就是Perplexity
- 多次尝试，调优perplexity-topic number曲线
    - 困惑度越小，越容易过拟合
    - 某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg)
- K × 词典的大小 约等于 语料库中词的总数，这个是老师上课说的，但是实锤无效。\alpha的经验选择为50/k, 其中k是topic数目，beta一般为0.01

# LDA和Word2Vec区别？LDA和Doc2Vec区别？
- LDA比较是doc，word2vec是词
- LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示
    - LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息
- LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果

# LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?
- 通常alpha为1/k，k为类别数，beta一般为0.01
- alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确
- beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度
- chucksize大一些更新的过程比较平稳，收敛更加平稳

# 使用过LDA，你有什么问题？

- 这个算法本身的一些问题，比如
  - LDA 产生的主题往往被高频词占据，这种现象导致低频词在实际应用中的作用非常有限
  - 词袋模型的通病，顺便还可以考一下词袋模型有哪些
  - 泛化能力差，对新词的接受程度不如一些字级别的算法强
- 实际使用的时候要注意的问题，比如
  - 短文本效果差。lda用的是近似统计推断的方式，通过观察单个词所属的主题分布推断整句所属的主题，短文本中的词个数少，观测结果少，得到的结果不置信，也就是数据sparisty问题
  - 主题个数太黑盒，连预估的搜索区间都没办法很好给出，取决于文章属性，文章质量，以及最后想得到的主题粗细力度
  - 对通识性的内容效果好，对突发性的内容效果差。可以识别法律相关的内容，因为它们不咋变化，聚出来的可以长时间复用；但是对热点资讯就不好，因为热点是一阵一阵的，很难长久化复用

# 你用真实用过吗？对比过效果吗？
- 迭代次数在1000-1200次基本上均可收敛，亲测了20万条的小样本数据和1亿2000万条的大样本数据均在1100-1400收敛，loss提升小于十万分之一
- 不要用变分推断，实测吉布斯采样的warplda，lightlda效果比GensimLDA要好，聚出来的主题一致性更高
- 如果内容偏向单一主题的场景，比如信息流中的新闻，通常只讲一件事，alpha建议小一点，建议0.1；如果是传统的小说散文法律文本，通常会有很多思想不能简单化为一个主题的时候，alpha建议大一点，建议10

# 超参数\alpha \beta对训练的影响？

\alpha越大，先验起的作用就越大，推导的topic分布就越倾向于在每个topic上的概率都差不多。\alpha的经验选择为50/k, 其中k是topic数目，beta一般为0.01

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gip21g3g9rj30ic07tn1t.jpg)

3个彩图中，每一个点对应一个主题分布，高度代表某个主题分布被dirichlet分布选中的概率，且选不同的alpha，dirichlet 分布会偏向不同的主题分布

# LDA你会有哪些常规的预处理步骤？
- 去除一些TF/DF较低/高的词，较低的词在拟合的过程中会被平滑掉，较高的词没有区分力
- 手动合并2-gram的一些词，会增强生成主题的表述能力
- 可以通过相似文本合并，相似用户合并，解决短文本主题生成不准的问题，如果数据量很大则不用考虑，直接剔除短文本即可
- 数据分布不平衡会导致小主题识别不出来。原因是LDA设计思路就是使得通过主题-词分布+文本-主题分布矩阵最大化语料库词出现的似然概率，如果某个类别特别大，小主题可以拆开单独训练一个model

# LDA的最大似然不好求的原因？为什么不直接用EM？为什么LDA引入了一堆数学理论？

LDA的最大似然不好求的原因是引入了主题的概率先验，alpha和beta需要求，且还有考虑每个主题生成词的概率，通常求隐藏变量的EM算法需要条件概率，但是LDA的条件概率难以描述。所以用的是吉布斯得到的几个样本来近似这个条件分布替代精准求解。

